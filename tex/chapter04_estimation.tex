% !TEX root = ../pdf/stat205.tex
% [There are multiple stat205.tex files, but the one in ../pdf is the usual one]



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Estimation}


\begin{verse}{\it
``When there are but two players, your theory which proceeds by combinations is very just. \\
But when there are three, I believe I have a proof that it is unjust that you should proceed in any other manner than the one I have.''\vspace*{6pt}} \\
\hspace*{2cm} -- Pascal's letter to Fermat\FOOTNOTE{from \url{https://www.york.ac.uk/depts/maths/histstat/pascal.pdf}}
\end{verse}
\vspace*{12pt}


\section{Introduction}

In previous chapters, we worked with problems where the sample space was fully known.
For instance, in Examples \autoref{exmp:coin_toss} and \autoref{exmp:heads_observe}, we knew that the coin was fair and that it could only land on heads or tails.
In Example \autoref{exmp:three_balls_urn}, we had complete information about the number of balls in the urn and their respective colors.
Example \autoref{exmp:eye_color} provided us with the exact frequency distribution of eye colors among town residents.
In Example \autoref{exmp:women_heights}, we were given that women's heights in the city followed a normal distribution, and so on.

In this chapter, we will work with statistical problems.
Unlike in previous examples, we do not know the bigger picture in these cases.
However, we have some information that comes from this bigger picture.
For example, we can draw balls from an urn without being able to see what's inside.
Or we might randomly select some women from Example \autoref{exmp:women_heights}, measure their heights, and use these observations to learn about their height distribution.

As another example, consider tossing a coin 500 times when we don't know whether it's fair or biased.
Suppose we get 243 heads.
We might then estimate a parameter such as \( p \), the probability of heads, as \( \frac{243}{500} = 0.486 \) using this frequentist approach.
However, this approach involves uncertainty.
For instance, another experiment might yield 225 heads, or someone might use a different method than the frequentist approach to estimate \( p \).
Therefore, the estimate of \( p \) cannot be determined with perfect accuracy.
In statistics, we study methods and principles to find the best possible estimations for unknown parameters like \( p \).

In this section, we'll apply what we learned previously to develop statistical inference.
First, we'll establish some key terminology.

\section{Estimation of Parameters}

Consider a discrete or continuous random variable  \( X \).
For example, \( X \) could be the number of children in households in a city (a discrete random variable) or the height of citizens in the city (a continuous random variable).
In the first case, it is defined for households, and in the second, it is defined for individuals.

Alternatively, \( X \) can represent the outcome of a random experiment.
For instance, when tossing a coin, the result of interest could be the outcome of the coin toss (a discrete random variable) or the time it takes for the coin to land after leaving the hand (a continuous random variable).

The studied instances (households, individuals, trials of a random experiment, etc.) are called the \keyterm{population}.
The population size is typically denoted by \( N \).

Suppose we know the PDF of \( X \), but it depends on one or more unknown parameters.
For instance, in a Bernoulli trial, \( X \) follows a Bernoulli distribution with unknown parameter \( p \).
While it would be ideal to record the variable of interest for every member of the population to determine \( p \), this is often impractical.
The population may be too large, making such calculations time-consuming, impossible, or expensive.
Instead, we gather information from a smaller subset of the population, called a \keyterm{sample}.
The sample size is typically denoted by \( n \).

\subsection{Statistic}

Suppose \( X \) follows a distribution \( f(x; \theta) \), where \( \theta \in A \) is an unknown \keyterm{paramter}.
The set \( A \) of all possible values for \( \theta \) is called the \keyterm{parameter space}.
For example, if \( X \) is a Bernoulli variable with success probability \( \theta \),
then \( 0 \leq \theta \leq 1 \), and so \( A = [0, 1] \) is the parameter space.


The parameter may also be a \( k \)-dimensional vector, denoted \( \bm{\theta} = (\theta_1, \theta_2, \ldots, \theta_k) \).
For instance in normal distribution, \( \theta = (\mu, \sigma) \) is a two-dimensional parameter.

A \keyterm{statistic} is a function \( U = g(X_1, X_2, \ldots, X_n) \) of a random sample \( X_1, X_2, \ldots, X_n \) that does not depend on unknown parameters.
By definition, a statistic is itself a random variable (since it is a function of random variables).
The distribution of \( U \) may or may not depend on the parameters.
For example, if \( X_1, X_2, \ldots, X_n \) is a random sample from \( N(\mu, 1) \),
then the sample mean \( U = \bar{X} = \frac{X_1 + X_2 + \ldots + X_n}{n} \) follows a normal distribution \( N(\mu, \frac{1}{n}) \) by the Central Limit Theorem (CLT).
Here, the distribution of \( U \) depends on the mean parameter \( \mu \).

\subsection{Estimator and Estimate}

If we observe the values \( x_1, x_2, \ldots, x_n \) for the random sample \( X_1, X_2, \ldots, X_n \)
and compute the statistic \( u = g(x_1, x_2, \ldots, x_n) \),
we can use \( u \) as an estimation of the parameter \( \theta \).
In this context, \( U = g(X_1, X_2, \ldots, X_n) \) is called an \keyterm{estimator} of \( \theta \),
while the computed value \( u \) is called an \keyterm{estimate} of \( \theta \).

Note that an estimator is itself a statistic and is therefore a random variable.
Thus, both estimators and statistics are denoted by capital letters.
In contrast, estimates (observed values) are denoted by lowercase letters.
Also, while every estimator for \( \theta \) is a statistic,
not every statistic serves as an estimator for \( \theta \).

\begin{exmp}
    Suppose \( X \) represents a country's annual GDP growth, where \( X \sim N(\mu, 0.2) \).
    We want to estimate \( \mu \) based on this random sample:
    \[ 1.1, 1.6, 2.4, 3.2 \]
    Since \( \mu \) is the mean of \( X \), observing multiple realizations of \( X \) should allow us to estimate \( \mu \).
    For instance, the sample mean of these observations seems like a proper estimate for \( \mu \):
    \begin{gather*}
        u = \bar{x} = \frac{1.1 + 1.6 + 2.4 + 3.2}{4} = 2.075
    \end{gather*}
    The corresponding eastimator is:
    \begin{gather*}
        U = \bar{X} = \frac{X_1 + X_2 + X_3 + X_4}{4}
    \end{gather*}
\end{exmp}
\begin{exmp}
    Consider a population of \( N = 1423 \) accident-involved individuals, where \( p \) denotes the proportion who excessively drink.
    The proportion of them in a random sample \( \hat(p) \) may be used to estimate \( p \).
\end{exmp}
In the previous two examples, \( \bar{X} \) and \( \hat{p} \) are called \keyterm{point estimators}.
Deriving point estimators is one way of estimating population parameters.
Ideal point estimators possess some characteristics.
For instance, we want low variance, meaning the point estimates don't vary widely across different samples.

While various methods exist for deriving point estimators, we focus instead on confidence intervals â€“ interval estimates that capture the parameter of interest with a specified level of confidence.

\section{Confidence Intervals}

To better uderstand confidence intervals, let's start with an example:

\begin{exmp}
    Let \( X_1, X_2, \ldots, X_100 \) be a random sample from \( X \sim N(\mu, 9) \).
    What is the probability that the unknown parameter \( \mu \) lies in the interval \( (\bar{X} - 0.3, \bar{X} + 0.3) \)?
\end{exmp}
\begin{solution}
    We know from the Central Limit Theorem that \( \bar{X} \sim N(\mu, 0.09) \),
    and therefore \( \frac{\bar{X} - \mu}{0.3} \sim N(0, 1) \). Thus:
    \begin{align*}
        P(\bar{X} - 0.3 < \mu < \bar{X} + 0.3) &= P(-1 < \frac{\bar{X} - \mu}{0.3} < 1)\\
        &= P(-1 < Z < 1)
    \end{align*}
    which equals:
    \begin{lstlisting}[language=R]
> pnorm(1) - pnorm(-1)
[1] 0.6826895
    \end{lstlisting}
\end{solution}
Note that we call the interval \( (\bar{X} - 0.3, \bar{X} + 0.3) \) in the previous example a \emph{random interval} since it has two random variables as its endpoints.
This random interval contains the parameter \( \mu \) with approximately 68\% probability.
From a frequentist perspective, this implies that if we were to construct 100 such intervals from different random samples, we would expect about 68 of them to contain the true parameter \( \mu \).
For example, one such interval might be \( (1.6, 2.2) \) when the sample mean is \( \bar{x} = 1.9 \).
We call such an interval a 68\% confidence interval.
But note that this does not mean \( (1.6, 2.2) \) contains the parameter \( \mu \) with 68\% probability!
\( (1.6, 2.2) \) is an observed interval, and it already either cotains \( \mu \) or not.

Let \( U \) be a point estimator for \( \theta \) based on a random sample from \( f(x; \theta) \).
Consider the random interval \( (g_1(U), g_2(U)) \), where \( g_1(U) \) and \( g_2(U) \) are functions of \( U \).
If
\begin{gather*}
    P(g_1(U) < \theta < g_2(U)) = 1 - \alpha
\end{gather*}
for some constant \( 0 \leq \alpha \leq 1 \) (independent of \( \theta \)),
then this interval is called a \keyterm{\( 100(1 - \alpha)\% \) confidence interval} (or a confidence interval with confidence level \( 1 - \alpha \)) for the parameter \( \theta \).

As we will later see, the structure of our confidence intervals are
\begin{gather*}
    (\text{population estimator} - \text{margin of error}, \text{population estimator} + \text{margin of error})
\end{gather*}
As the confidence level \( 1 - \alpha \) approaches 1 and the average margin of error \( E(\text{margin of error}) \) decreases,
we gain both higher confidence that the interval contains the true parameter value and a more precise (narrower) interval estimate.